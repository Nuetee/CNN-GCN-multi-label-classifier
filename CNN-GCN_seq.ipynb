{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m product\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\__init__.py:138\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    121\u001b[0m     concat,\n\u001b[0;32m    122\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     qcut,\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[1;32m--> 138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m testing  \u001b[39m# noqa:PDF015\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_print_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     \u001b[39m# excel\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     ExcelFile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m     read_spss,\n\u001b[0;32m    172\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\testing.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mPublic testing utility functions.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_testing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m      8\u001b[0m     assert_frame_equal,\n\u001b[0;32m      9\u001b[0m     assert_index_equal,\n\u001b[0;32m     10\u001b[0m     assert_series_equal,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_extension_array_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_frame_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_series_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_index_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_testing\\__init__.py:903\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[0;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(expected_exception, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# noqa: PDF010\u001b[39;00m\n\u001b[1;32m--> 903\u001b[0m cython_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39m_cython_table\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    906\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[0;32m    907\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[39m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[39m    keys and expected result.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLabelsByFrequency(artworks, frequency):\n",
    "  for column in artworks.filter(regex='labels'):\n",
    "    label_freq = artworks[column].apply(\n",
    "        lambda s: [x for x in s]).explode().value_counts().sort_values(ascending=False)\n",
    "\n",
    "    # Create a list of rare labels\n",
    "    rare = list(label_freq[label_freq < frequency].index)\n",
    "\n",
    "    artworks[column] = artworks[column].apply(\n",
    "        lambda s: [x for x in s if x not in rare])\n",
    "    artworks[column] = artworks[column].apply(\n",
    "        lambda s: s if len(s) != 0 else np.nan)\n",
    "\n",
    "  artworks.dropna(inplace=True)\n",
    "  return artworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39martworks\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m----> 2\u001b[0m     artworks \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"artworks\", \"rb\") as file:\n",
    "    artworks = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artworks = filterLabelsByFrequency(artworks, 50)\n",
    "\n",
    "object_label_freq = artworks['object_labels'].apply(\n",
    "    lambda s: [x for x in s]).explode().value_counts()\n",
    "style_label_freq = artworks['style_labels'].apply(\n",
    "    lambda s: [x for x in s]).explode().value_counts()\n",
    "material_label_freq = artworks['material_labels'].apply(\n",
    "    lambda s: [x for x in s]).explode().value_counts()\n",
    "\n",
    "label_freq = pd.concat([object_label_freq, style_label_freq,\n",
    "                       material_label_freq]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(label_list):\n",
    "    total_labels = []\n",
    "    for labels in label_list:\n",
    "      labels = list(filter(None, labels))\n",
    "      total_labels.extend(labels)\n",
    "\n",
    "    return len(list(set(total_labels))), list(set(total_labels))\n",
    "\n",
    "\n",
    "def get_index_of_np(numpy, val):\n",
    "  index_np = np.where(numpy == val)[0]\n",
    "\n",
    "  if index_np.size != 1:\n",
    "    print('a', index_np[0], index_np.size, val)\n",
    "    return None\n",
    "  else:\n",
    "    return index_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(artwork_labels_list, total_labels, N_LABEL):\n",
    "  adjacency_matrix = np.zeros((N_LABEL, N_LABEL))\n",
    "\n",
    "  for artwork_labels in artwork_labels_list:\n",
    "    label_combinations = list(product(artwork_labels, repeat=2))\n",
    "\n",
    "    for label_combination in label_combinations:\n",
    "      pivot_index = get_index_of_np(total_labels, label_combination[0])\n",
    "      target_index = get_index_of_np(total_labels, label_combination[1])\n",
    "      if (pivot_index is None) or (target_index is None):\n",
    "        return None\n",
    "      else:\n",
    "        adjacency_matrix[pivot_index][target_index] += 1\n",
    "\n",
    "  for row_index, row in enumerate(adjacency_matrix):\n",
    "    basis = row[row_index]\n",
    "    for col_index, val in enumerate(row):\n",
    "      row[col_index] = val/basis\n",
    "\n",
    "  return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_graph(artworks):\n",
    "  label_classes = ['object_labels', 'style_labels', 'material_labels']\n",
    "  artwork_labels_list = []\n",
    "  labels_list_by_class = [set() for _ in range(3)]\n",
    "\n",
    "  for index, label_class in enumerate(label_classes):\n",
    "    labels_group = artworks[label_class]\n",
    "\n",
    "    if index == 0:\n",
    "      for labels in labels_group:\n",
    "        artwork_labels_list.append(copy.deepcopy(labels))\n",
    "        labels_list_by_class[index] = labels_list_by_class[index].union(\n",
    "            set(copy.deepcopy(labels)))\n",
    "    else:\n",
    "      for i, labels in enumerate(labels_group):\n",
    "        artwork_labels_list[i].extend(copy.deepcopy(labels))\n",
    "        labels_list_by_class[index] = labels_list_by_class[index].union(\n",
    "            set(copy.deepcopy(labels)))\n",
    "\n",
    "  N_LABEL, total_labels = count_labels(artwork_labels_list)\n",
    "  adjacency_matrix = get_adjacency_matrix(\n",
    "      artwork_labels_list, np.array(total_labels), N_LABEL)\n",
    "\n",
    "  class_one_hot_encoding_matrix = np.zeros((N_LABEL, 3))\n",
    "\n",
    "  for i in range(N_LABEL):\n",
    "    label = total_labels[i]\n",
    "    for j in range(3):\n",
    "      class_one_hot_encoding_matrix[i][j] = np.any(\n",
    "          np.array(list(labels_list_by_class[j])) == label)\n",
    "\n",
    "  embedding = nn.Embedding(N_LABEL, embedding_dim=5)\n",
    "  input = torch.tensor(range(len(total_labels)))\n",
    "  word_embedding_matrix = embedding(input).detach().cpu().numpy()\n",
    "\n",
    "  feature_matrix = np.concatenate(\n",
    "      (class_one_hot_encoding_matrix, word_embedding_matrix), axis=1)\n",
    "\n",
    "  return adjacency_matrix, feature_matrix, N_LABEL, total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix, feature_matrix, N_LABEL, total_labels = convert_to_graph(\n",
    "    artworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_tensor = torch.Tensor(\n",
    "    feature_matrix).to(torch.float32)\n",
    "adjacency_matrix_tensor = torch.Tensor(\n",
    "    adjacency_matrix).to(torch.float32)\n",
    "degree_matrix_tensor = torch.diag(torch.sum(adjacency_matrix_tensor, dim=1))\n",
    "laplacian_matrix_tensor = degree_matrix_tensor - adjacency_matrix_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, head_num=2):\n",
    "    super(AttentionModule, self).__init__()\n",
    "\n",
    "    self.head_num = head_num\n",
    "    self.attention_dim = out_dim // head_num\n",
    "\n",
    "    self.linears = nn.ModuleList()\n",
    "    self.correlations = nn.ParameterList()\n",
    "    for i in range(self.head_num):\n",
    "      self.linears.append(nn.Linear(in_dim, self.attention_dim))\n",
    "      correlation = torch.FloatTensor(\n",
    "          self.attention_dim, self.attention_dim).to(device)\n",
    "      nn.init.xavier_uniform_(correlation)\n",
    "      self.correlations.append(nn.Parameter(correlation))\n",
    "\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.set_out_dim = nn.Linear(head_num * self.attention_dim, out_dim)\n",
    "\n",
    "  def forward(self, x, adj):\n",
    "    heads = list()\n",
    "    for i in range(self.head_num):\n",
    "      x_i = self.linears[i](x)\n",
    "      alpha = self.attention_matrix(x_i, self.correlations[i], adj)\n",
    "      x_head = torch.matmul(alpha, x_i)\n",
    "      heads.append(x_head)\n",
    "    out = torch.cat(heads, dim=2)\n",
    "    out = self.set_out_dim(out)\n",
    "    return out\n",
    "\n",
    "  def attention_matrix(self, x_i, correlation, adj):\n",
    "    x = torch.matmul(x_i, correlation)\n",
    "    alpha = torch.matmul(x, torch.transpose(x_i, 1, 2))\n",
    "    alpha = torch.mul(alpha, adj)\n",
    "    alpha = self.tanh(alpha)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "\tdef __init__(self, in_dim, out_dim, act=None, atn=False, head_num=2, drop_rate=0):\n",
    "\t\tsuper(GCNLayer, self).__init__()\n",
    "\n",
    "\t\tself.linear = nn.Linear(in_dim, out_dim)\n",
    "\t\tnn.init.xavier_uniform_(self.linear.weight)\n",
    "\t\tself.activation = act\n",
    "\t\tself.use_attention = atn\n",
    "\t\tself.attention = AttentionModule(out_dim, out_dim, head_num)\n",
    "\t\tself.drop_rate = drop_rate\n",
    "\t\tself.dropout = nn.Dropout(self.drop_rate)\n",
    "\n",
    "\tdef forward(self, x, adj):\n",
    "\t\tout = self.linear(x)\n",
    "\t\tif self.use_attention:\n",
    "\t\t\tout = self.attention(out, adj)\n",
    "\t\tout = torch.matmul(adj, out)\n",
    "\t\tif self.activation != None:\n",
    "\t\t\tout = self.activation(out)\n",
    "\t\tif self.drop_rate > 0:\n",
    "\t\t\tout = self.dropout(out)\n",
    "\t\treturn out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super(GatedSkipConnection, self).__init__()\n",
    "\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "\n",
    "    self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "    self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "    self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, in_x, out_x):\n",
    "    if (self.in_dim != self.out_dim):\n",
    "        in_x = self.linear(in_x)\n",
    "    z = self.gate_coefficient(in_x, out_x)\n",
    "    out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "    return out\n",
    "\n",
    "  def gate_coefficient(self, in_x, out_x):\n",
    "    x1 = self.linear_coef_in(in_x)\n",
    "    x2 = self.linear_coef_out(out_x)\n",
    "    return self.sigmoid(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "  def __init__(self, n_layer, in_dim, hidden_dim, out_dim, sc='gsc', act=None, atn=True, head_num=2, drop_rate=0):\n",
    "    super(GCNBlock, self).__init__()\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    for i in range(n_layer):\n",
    "        self.layers.append(GCNLayer(in_dim if i == 0 else hidden_dim,\n",
    "                                    out_dim if i == n_layer-1 else hidden_dim,\n",
    "                                    act,\n",
    "                                    False if i == n_layer-1 else atn,\n",
    "                                    1 if i == n_layer-1 else head_num,\n",
    "                                    drop_rate))\n",
    "    if sc == 'gsc':\n",
    "        self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "    elif sc == 'no':\n",
    "        self.sc = None\n",
    "    else:\n",
    "        assert False, \"Wrong sc type.\"\n",
    "\n",
    "  def forward(self, x, adj):\n",
    "    residual = x\n",
    "    out = None\n",
    "    for i, layer in enumerate(self.layers):\n",
    "        out, adj = layer((x if i == 0 else out), adj)\n",
    "    if self.sc != None:\n",
    "        out = self.sc(residual, out)\n",
    "    return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "  def __init__(self, n_layer, in_dim, hidden_dim, out_dim, sc='gsc', act=None, atn=True, head_num=2, drop_rate=0):\n",
    "    super(InceptionModule, self).__init__()\n",
    "    self.GCNBlocks = nn.ModuleList()\n",
    "    for i in range(n_layer):\n",
    "      self.GCNBlocks.append(GCNBlock(i + 1,\n",
    "                                     in_dim,\n",
    "                                     hidden_dim,\n",
    "                                     out_dim,\n",
    "                                     sc,\n",
    "                                     act,\n",
    "                                     atn,\n",
    "                                     head_num,\n",
    "                                     drop_rate))\n",
    "\n",
    "  def forward(self, x, adj):\n",
    "    outs = []\n",
    "    for i, block in enumerate(self.GCNBlocks):\n",
    "      out, _ = block(x, adj)\n",
    "      outs.append(out)\n",
    "\n",
    "    feature_concatenate = torch.cat(outs, 2)\n",
    "    return feature_concatenate, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        out = torch.squeeze(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "  def __init__(self, args, adj):\n",
    "    super(GCNNet, self).__init__()\n",
    "\n",
    "    self.InceptionModule = InceptionModule(args.n_layer,\n",
    "                                           args.in_dim,\n",
    "                                           args.hidden_dim,\n",
    "                                           args.out_dim,\n",
    "                                           args.sc,\n",
    "                                           args.act,\n",
    "                                           args.atn,\n",
    "                                           args.head_num,\n",
    "                                           args.drop_rate)\n",
    "    self.Readout = ReadOut(args.out_dim * args.n_layer,\n",
    "                           1, None)\n",
    "    self.adj = adj\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, _ = self.InceptionModule(x, self.adj)\n",
    "    out = self.Readout(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, n_label):\n",
    "        super(Resnet50, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(num_ftrs, n_label)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNGCN_2(nn.Module):\n",
    "\tdef __init__(self, num_classes, adj, args):\n",
    "\t\tsuper(CNNGCN_2, self).__init__()\n",
    "\t\tself.cnn = Resnet50(num_classes)\n",
    "\t\tself.gcnnet = GCNNet(args, adj)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tcnn_out = self.cnn(x)\n",
    "\t\tcnn_out_2 = torch.transpose(torch.unsqueeze(cnn_out, 1), 1, 2)\n",
    "\t\tgcn_out = self.gcnnet(cnn_out_2)\n",
    "\t\treturn gcn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CNNGCN_2.__init__() missing 2 required positional arguments: 'adj' and 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./models/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m pytorch_model \u001b[39m=\u001b[39m CNNGCN_2(\u001b[39m333\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m pytorch_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m./models/cnngnn-epoch100_3_1_5_1_gsc_elu_true_2_0-earlystop_5.pt\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m pytorch_model\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;31mTypeError\u001b[0m: CNNGCN_2.__init__() missing 2 required positional arguments: 'adj' and 'args'"
     ]
    }
   ],
   "source": [
    "model_path = './models/'\n",
    "pytorch_model = CNNGCN_2(333)\n",
    "pytorch_model.load_state_dict(torch.load(\n",
    "    './models/cnngnn-epoch100_3_1_5_1_gsc_elu_true_2_0-earlystop_5.pt'))\n",
    "pytorch_model.eval()\n",
    "pytorch_model\n",
    "dummy_input = torch.zeros(1, 3, 224, 224)\n",
    "torch.onnx.export(pytorch_model, dummy_input,\n",
    "                  './models/cnngnn_3_1_5_1_gsc_elu_true_2_0.onnx', verbose=True, opset_version=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
